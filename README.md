Google Cloud Platform

# Table of contents

- [Table of contents](#table-of-contents)
- [GCP setup for access](#gcp-setup-for-access)
  - [GCP initial setup (Create Project and ServiceAccount)](#gcp-initial-setup-create-project-and-serviceaccount)
  - [GCP setup for access](#gcp-setup-for-access-1)
  - [Set up Cloud VM and SSH Access](#set-up-cloud-vm-and-ssh-access)
  - [Login to VM](#login-to-vm)
  - [Create config file for quick VM access](#create-config-file-for-quick-vm-access)
  - [Access remote VM with local VS Code](#access-remote-vm-with-local-vs-code)
  - [Install Anaconda on VM](#install-anaconda-on-vm)
  - [Download entire zoomcamp repo to VM](#download-entire-zoomcamp-repo-to-vm)
  - [Install Docker on VM](#install-docker-on-vm)
  - [Install Terraform](#install-terraform)
  - [Install SPARK](#install-spark)
  - [Add new Disk to Cloud VM](#add-new-disk-to-cloud-vm)

# GCP setup for access

## GCP initial setup (Create Project and ServiceAccount)

GCP is organized around _projects_. You may create a project and access all available GCP resources and services from the project dashboard.

We will now create a project and a _service account_, and we will download the authentication keys to our computer. A _service account_ is like a user account but for apps and workloads; you may authorize or limit what resources are available to your apps with service accounts.

Please follow these steps:

1. Create an account on GCP. You should receive $300 in credit when signing up on GCP for the first time with an account.
1. Setup a new project and write down the Project ID.
    1. From the GCP Dashboard, click on the drop down menu next to the _Google Cloud Platform_ title to show the project list and click on _New project_.
    1. Give the project a name. We will use `your-project-name` in this example. You can use the autogenerated Project ID (this ID must be unique to all of GCP, not just your account). Leave the organization as _No organization_. Click on _Create_.
    1. Back on the dashboard, make sure that your project is selected. Click on the previous drop down menu to select it otherwise.
1. Setup a service account for this project and download the JSON authentication key files.
    1. _IAM & Admin_ > _Service accounts_ > _Create service account_
    1. Provide a service account name. We will use `data-engineering-user`. Leave all other fields with the default values. Click on _Create and continue_.
    1. Grant the Viewer role (_Basic_ > _Viewer_) to the service account and click on _Continue_
    1. There is no need to grant users access to this service account at the moment. Click on _Done_.
    1. With the service account created, click on the 3 dots below _Actions_ and select _Manage keys_.
    1. _Add key_ > _Create new key_. Select _JSON_ and click _Create_. The files will be downloaded to your computer. Save them to a folder and write down the path.
1. Download the GCP SDK for local setup. Follow the instructions to install and connect to your account and project.
1. Set the environment variable to point to the auth keys.
    1. The environment variable name is `GOOGLE_APPLICATION_CREDENTIALS`
    1. The value for the variable is the path to the json authentication file you downloaded previously.
    1. Check how to assign environment variables in your system and shell. In bash, the command should be:
        ```bash
        export GOOGLE_APPLICATION_CREDENTIALS="<path/to/authkeys>.json"
        ```
    1. Refresh the token and verify the authentication with the GCP SDK:
        ```bash
        gcloud auth application-default login
        ```

You should now be ready to work with GCP.

## GCP setup for access

In the following chapters we will setup a _Data Lake_ on Google Cloud Storage and a _Data Warehouse_ in BigQuery. We will explore these concepts in future lessons but a Data Lake is where we would usually store data and a Data Warehouse provides a more structured way to access this data.

We need to setup access first by assigning the Storage Admin, Storage Object Admin, BigQuery Admin and Viewer IAM roles to the Service Account, and then enable the `iam` and `iamcredentials` APIs for our project.

Please follow these steps:

1. Assign the following IAM Roles to the Service Account: Storage Admin, Storage Object Admin, BigQuery Admin and Viewer.
    1. On the GCP Project dashboard, go to _IAM & Admin_ > _IAM_
    1. Select the previously created Service Account and edit the permissions by clicking on the pencil shaped icon on the left.
    1. Add the following roles and click on _Save_ afterwards:
        * `Storage Admin`: for creating and managing _buckets_.
        * `Storage Object Admin`: for creating and managing _objects_ within the buckets.
        * `BigQuery Admin`: for managing BigQuery resources and data.
        * `Viewer` should already be present as a role.
1. Enable APIs for the project (these are needed so that Terraform can interact with GCP):
   * Navigate to APIs & Services > Library and search for "IAM Service Account Credentials API" and "Identity and Access Management (IAM) API"
   * Enable both APIs
1. Make sure that the `GOOGLE_APPLICATION_CREDENTIALS` environment variable is set.

## Set up Cloud VM and SSH Access

Now let's set up a cloud VM instance and SSH access.

1. In GCP, go to Compute Instances > VM Instance. Enable the Compute Engine API if prompted. 

1. We don't currently have any VM instances. Before we create one, we will need to create an SSH key (we'll use this to securely login to the instance)

    * Create SSH directory in root folder `mkdir ~/.ssh`
    * In the SSH directory, run the command `ssh-keygen -t rsa -f gcp -C <yourname> -b 2048`

        * It's important to distinguish between public key and private key - we need to upload the public key
        * Private key is your security credential and must be kept strictly confidential.
        * Public key is used for registration and verification, and can be shared publicly.
        * Private key encrypts and generates signatures, public key verifies signatures, ensuring only those with the private key can access resources.
        * View public key (remember to distinguish between public and private keys - upload the public key): use `cat gcp.pub`

    * Don't bother with passphrases to save us having to always type this in (just hit return on both prompts)
    * If you type `ls` you can now see that two keys have been generated, a public and a private key
    * DO NOT SHARE PRIVATE KEY WITH ANYONE
1. Next, put public key to Google Cloud. Under Compute Engine go to Metadata. Select SSH tab, and add SSH key. Then enter the key found within the public key file and save.
    * So now, all VMs we create will have this key
1. Next create a VM instance on the previous screen
    * Give it a name
    * Select a region close by to you
    * I selected the `e2-standard-4` machine type.
    * Scroll down to Boot Disk. I changed this to `ubuntu`, `ubuntu 20.04 LTS` and gave it `30 GB`. 
    * Click select, then click Create.

## Login to VM
Now copy the `External IP`, head to terminal and enter the below:

`ssh -i <key path> <yourname>@<externalip>`

For example:
ssh -i /c/Users/[user name]/.ssh/gcp [user name]@[external_ip]
* The yourname here is the SSH Key username, which can be found in metadata

`externalip` is the ip you just copied and `yourname` is the name you gave when ran the `ssh-keygen` command.

Now we're connected to our VM. If your run the `htop` command you can get some details. Run `ctrl + c` to exit. 

## Create config file for quick VM access (note config file creation location)

Now create a file under `~/.ssh` called config. We'll use this for configuring SSH.
> Note the path creation requirements - it must be created according to requirements, otherwise it will report errors
> 
> The ~/.ssh/config file is your local computer's SSH client configuration file, which should be created in your local user's .ssh directory, not in the Google Cloud VM folder. This file tells your local SSH client how to connect to remote servers.
> 
> The path is C:\Users\<your username>\.ssh\config (for Windows).

In that config file, write the following:
```
Host data-engineering-vm
    HostName <external_ip>
    User <yourname>
    IdentityFile ~/.ssh/gcp
```
You can name it what you want. I just used `data-engineering-vm`. 

Once you've saved this, entering your VM from the command line is as simple as running from your home directory 

`ssh data-engineering-vm`

If we want to logout, we can use `ctrl + d`, but stay in it for now.

## Access remote VM with local VS Code

Next let's configure VS Code to access our VM. 

Install the `Remote - SSH` extension. Then from the command palette search and select Connect to Host (Remote-SSH) and select data-engineering-vm (this appears because we created the config file earlier)

## Install Anaconda on VM

Next up, we'll download anaconda on our VM using `wget <link to anaconda linux installation file>`. And then `bash <downloaded file>`. Run through the installation steps.
* Note to download the Linux version
* You can use `conda --version` to check the version and see if the download was successful
* You can use `rm Anaconda3-2024.10-1-Linux-x86_64.sh` to delete the installation package
* After installing Anaconda, Python will be automatically installed, and Anaconda comes with many scientific computing and data analysis libraries, such as numpy, pandas, scipy, matplotlib, etc. You can enter `python --version` to check the Python version
* Anaconda comes with Jupyter Notebook. After installing Anaconda, you can directly enter `jupyter notebook` to start Jupyter Notebook without additional installation.
* Note: if you're running Jupyter Notebook on a remote machine, you will need to redirect ports 8888 for Jupyter Notebook 

## Common Commands

1. Process Management and System Monitoring
    - `htop`: An interactive process viewer and system monitoring tool for Unix/Linux systems.
    - `q`: Exit `htop`.

2. Directory and File Operations
    - `cd /tmp`: Change to `/tmp` directory.
    - `ls`: List files in current directory.

3. Software Installation and Environment Management
    - **Download Anaconda**:
    ```bash
    wget https://repo.anaconda.com/archive/Anaconda3-2024.10-1-Linux-x86_64.sh
    chmod +x Anaconda3-2024.10-1-Linux-x86_64.sh 
    bash Anaconda3-2024.10-1-Linux-x86_64.sh
    ```

4. Add Anaconda's bin directory to the PATH environment variable, enter the following, then add `export PATH="$HOME/anaconda3/bin:$PATH"`

    ```bash
        nano ~/.bashrc
    ```
     Reload the Bash configuration file
    ```bash
        source ~/.bashrc
    ```
5. Verify

    ```bash
        conda --version
        python --version
    ```

## Download entire zoomcamp repo to VM
Next up `git clone` the entire zoomcamp github repo to the VM (you are actually best to clone your own zoomcamp repo, assuming you've been following along).
This command can be entered in VS Code terminal or in terminal connected to VM
```bash
git clone https://github.com/DataTalksClub/data-engineering-zoomcamp.git
```

## Install Docker on VM

Let's install Docker on our VM now:

We first run:

`sudo apt-get update`

This will fetch the software packages.

Then we run:

`sudo apt-get install docker.io`

In this step, we'll find that we can't run docker on our VM even though it's installed. Permission will be denied.

Let's run some commands which will stop us having to use `sudo` everytime we have to run Docker. They basically add us to a docker group, giving us permissions.

`sudo groupadd docker`

`sudo gpasswd -a $USER docker`

`sudo service docker restart`

`sudo chmod 666 /var/run/docker.sock`

We also need to log out and log back in so that group membership is re-evaluated. 

Next, let's install `docker-compose`. 

Go to the Docker Compose releases page and find the latest `docker-compose-linux-x86_64` release.

Copy link, create folder called `bin` in your VM, change directories into it, and run `wget <the link> -O docker-compose`

This is an executable file, but our system doesn't know this. To let our system know, run this command:

`chmod +x docker-compose`

Now let's make it visible to every directory (not just the bin directory). 

Go back to home folder in VM, and run `nano .bashrc` (nano is a text editor).

Scroll to the bottom, and add bin directory to our path by writing this:

`export PATH="${HOME}/bin:${PATH}"`

This command is basically pre-pending something to the beginning of our PATH variable (not overwriting it).

Save and exit, and run `source .bashrc` to essentially refresh the changes.

Now if you run `which docker-compose` you can see that our system is now able to find it.

Next, try going to the docker sql directory and running `docker-compose up` (this will start your database containers)

With that (hopefully) working, head back to the home directory and install pgcli (the postgres command line utility) using `pip install pgcli`.

You'll find that if you try and use this, you'll get some error (although it'll still work). But there's a workaround. 

First uninstall it with `pip uninstall pgcli`

We'll now use anaconda to install it.

But first... go back to the VS Code instance you had open, where you were connected to the VM. Go to open folder, and select the project folder. This is you now working on your VM!

Now install pgcli with `conda install -c conda-forge pgcli`

Next, install mycli using `pip install -U mycli`. This will fix some potential issues with pgcli.

You can test connection to your database using something like (you should have run `docker-compose up` before this)

`pgcli -h localhost -U root -p 5432 -d ny_taxi` 

It'll probably be different for you. This was from a previous step. Make sure to use the correct port and database name.

Just another issue to note, if you try running `docker ps` and get a permission denied error, try running:

`sudo chmod 666 /var/run/docker.sock`

This seemed to resolve it. As far as I can tell, this allows all users to read and write the file/folder.

If we want to stop our docker containers, we can run `docker-compose down` from the docker directory. But let's keep them running for now.

TIP: use `docker-compose up -d` to start our containers and gain control back of our terminal, while it operates in the background.

What if we want to interact with our postgres instance locally? Well we can forward the port to our local machine.

In VS Code, go to the `PORTS` tab (next to `TERMINAL`). Click forward the port. It's probably 5432 for you. 

Once that's done, open up a terminal on your local machine and execute the same `pgcli` command from earlier. We can then do the same for the `8080` port.

We can also do this for Jupyter Notebook. Just run `jupyter notebook` in the terminal of our VM in VS Code. Add `8888` port on port tab. We can then use the link shown in the terminal to access Jupyter from our local machine. 

## Install Terraform
Next, lets install Terraform. Go to Terraform download page and copy link of `Amd64` binary, navigate to `~/bin` directory, and run:

`wget https://releases.hashicorp.com/terraform/1.1.4/terraform_1.1.4_linux_amd64.zip`

This will create a zip archive so we'll need to unzip it. 

First install the zip utility:

`sudo apt-get install unzip`

Then unzip the zip file:

`unzip <zipfile>`

and remove the zip file:

`rm *.zip`

Now, because `~/bin` is already discoverable, we can do stuff with Terraform from anywhere.

Before we do anything else, do you remember that JSON file containing your authentication key? Let's copy it to our VM.

We'll use `sftp` which stands for SSH File Transfer Protocol. This will let us securely send this file to our VM.

> ssh: Full-featured tool for managing remote servers.
>
> sftp: Focused on file transfer and management.
>
> scp: Simple tool for quick file copying.

We can connect directly to our VM using:

`sftp data-engineering-vm`

Run this from your local machine.

Once connected, create a new directory:

`mkdir .gc`

`cd .gc`

Then `put` our JSON file there:

`put <localfullpathtoJSON>`

Next, enter `exit` to exit sftp, re-login to VM normally, then run:

`export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/[your-service-account-key].json`

then:

`gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS`

What we've done is activated our service account credentials.

Let's now run through the terraform commands:

`terraform init`
`terraform plan`
`terraform apply`

What if I want to shut down my VM? Just run `sudo shutdown now` from the terminal. We can also do this from the GUI in GCP. 

We can start it again from the GCP console too. Once started, we need the external ip. We then need to go into our `~/.ssh/config` file and replace the hostname there with the new ip.

We then run `ssh data-engineering-vm` again to connect to our vm.

Now it's time to work on your projects! Note that you can create updated versions of your Docker images using `docker build -t image_name:v002 .`. The `v002` signifies this is an updated version of your original image.

## Install SPARK
Please refer to Batch Processing Spark documentation for detailed SPARK installation instructions.

## Add new Disk to Cloud VM
After adding a new Disk in GCP, you need to format and mount the new hard disk on the Linux VM

* `lsblk` to view all disks

* Step 1: Check if `sdb` (new disk) has partitions
```bash
lsblk /dev/sdb
```
If sdb has no partitions, create a partition (some commands will have problems with disks that don't have partitions created)

* Step 2: Create partition
If sdb is empty, you can use fdisk to create a new partition:
```bash
sudo fdisk /dev/sdb
```
In fdisk interactive mode, enter sequentially:

1) Enter n (new partition)
2) Press Enter (default primary partition)
3) Press Enter (default partition number 1)
4) Press Enter (default starting sector)
5) Press Enter (default use all space)
6) Enter w (write changes and exit)

Then run:
```bash
lsblk /dev/sdb
```
You should see sdb1 as the new partition.

* Step 3: Format the new partition
Format sdb1 as ext4 filesystem:
```bash
sudo mkfs.ext4 /dev/sdb1
```
If you want to use XFS filesystem:
```bash
sudo mkfs.xfs /dev/sdb1
```
* Step 4: Mount to /mnt/newdisk
1) Create mount directory:
```bash
sudo mkdir -p /mnt/newdisk
```
2) Mount disk:
```bash
sudo mount /dev/sdb1 /mnt/newdisk
```
3) Ensure disk is mounted successfully:
```bash
df -h
```
* Step 5: Make mount permanent
If you want sdb1 to automatically mount after system restart:

1) Get UUID
```bash
sudo blkid /dev/sdb1
```
Example output:
```bash
/dev/sdb1: UUID="abcd-1234" TYPE="ext4"
```
2) Edit /etc/fstab
```bash
sudo nano /etc/fstab
```
3) Add new line:

```bash
UUID=abcd-1234 /mnt/newdisk ext4 defaults 0 2
```
4) Save and exit (Ctrl + X → Y → Enter)

5) Test save success
```bash
cat /etc/fstab
```
6) Test fstab configuration
```bash
sudo mount -a
```
7) Create symbolic link for the disk
When executing ls in the /home directory, you cannot see the /mnt/newdisk directory, but you can enter it through cd. This is because: The /mnt/newdisk directory exists, but it is not a subdirectory of the /home directory. /mnt/newdisk is a mount point, it is actually a mount directory and will not appear in the ls output of the /home directory.

The cd behavior is based on absolute or relative paths, it will jump to the target directory without caring whether that directory appears in the ls output of the current path. So even though the /mnt/newdisk directory is not in the ls of /home, cd can still enter normally.
```bash
sudo ln -s /mnt/newdisk newdisk
```

> Additional information: Loop devices, your loop0, loop1, etc. are Loop devices mounted by Snap packages, no manual mounting or modification needed.
